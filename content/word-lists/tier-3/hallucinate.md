---

############################
### DO NOT CHANGE THESE VALUES ###
type: wordlist
layout: wordlist
outputs:
    - html
    - json
    - custom
############################

## YOU CAN CHANGE THE VALUES OF THE FIELDS BELOW ##
## DO NOT CHANGE THE FIELD NAMES ##

## Related terms, Recommended Replacements & Unsuitable Replacements fields expect a buletted list using - symbol ##

## Rationale, Status & Supporting Content allows multiline content, you can also use markdown format. Please make use the | symbol is not deleted.

title: "Hallucinate"

tier: 3
term: "hallucinate"
related_terms:
    - hallucination
use_context: "When generative AI fails to predict the correct outcome. It is actually a failure of the model, the data supplied to it, or in its reasoning."
definition: "An artificial intelligence (AI) application generates information that has no basis in fact."

recommendation: "Recommended to replace when possible."

recommended_replacements:
    - "inaccurate information (noun)"
    - "*create* or *generate* inaccurate information (verb)"
    - Factual error
    - Incorrect assertion
    - False positive
    - Distortion



unsuitable_replacements:
    - None

rationale: |
    AI "hallucination" is a misleading analogy and term that allows its LLM makers and owners to evade responsibility. There is also an anthropomorphic argument to be made for applying human traits to computer systems.

    When we build software and systems, we test them. When we find failures, we call them bugs. When we excuse our LLM models for "hallucinating" we do not learn why the failure happened and owners and makers do not take responsibility. Worse, this stigmatizes the mentally ill.

    Hallucinations -- perceptions that are not based in reality -- are also associated with mental illness or drug use. Using the term in a technology context, in either its noun or verb form, can be seen as insensitive to people who experience those conditions. 

status: | 
    N/A
supporting_content: | 
    N/A

---